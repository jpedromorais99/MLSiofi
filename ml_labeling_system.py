"""
Sistema de Rotula√ß√£o Semi-Supervisionada de Dados Or√ßament√°rios

Este sistema implementa:
1. Pr√©-processamento de texto das descri√ß√µes or√ßament√°rias
2. Vetoriza√ß√£o com TF-IDF (apenas caracter√≠sticas textuais)
3. Clustering com DBSCAN (vigil√¢ncia œÅ ‚â• 0.9) para agrupar itens similares
4. Exporta√ß√£o dos clusters para rotula√ß√£o manual
5. Aprendizado semi-supervisionado iterativo (ap√≥s rotula√ß√£o manual)
6. Visualiza√ß√£o e an√°lise de resultados

Fluxo de trabalho:
- Etapa 1: Gerar clusters de alta similaridade
- Etapa 2: Usu√°rio rotula manualmente alguns exemplos de cada cluster
- Etapa 3: Algoritmo propaga os r√≥tulos para dados n√£o rotulados
- Etapa 4: Treinar classificador com base rotulada
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√£o de visualiza√ß√£o
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

class SemiSupervisedBudgetLabeler:
    """
    Classe principal para rotula√ß√£o semi-supervisionada de dados or√ßament√°rios
    """
    
    def __init__(self, vigilance=0.9):
        """
        Inicializa o sistema de rotula√ß√£o
        
        Args:
            vigilance (float): Par√¢metro de vigil√¢ncia (similaridade m√≠nima) para clustering
        """
        self.vigilance = vigilance
        self.df = None
        self.features_matrix = None
        self.labels = None
        self.confidence_scores = None
        self.vectorizer = None
        self.label_mapping = {}
        self.iteration_history = []
        
    def load_data(self, filepath):
        """Carrega os dados do arquivo Excel"""
        print("üìä Carregando dados...")
        self.df = pd.read_excel(filepath)
        self.original_df = self.df.copy()
        print(f"‚úì {len(self.df)} registros carregados")
        return self
    
    def preprocess_text(self, text):
        """
        Pr√©-processa texto para an√°lise
        
        Args:
            text: Texto a ser processado
            
        Returns:
            Texto processado
        """
        if pd.isna(text):
            return ""
        
        text = str(text).upper()
        # Remove caracteres especiais mas mant√©m espa√ßos e letras/n√∫meros
        text = re.sub(r'[^\w\s]', ' ', text)
        # Remove espa√ßos m√∫ltiplos
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def create_features(self):
        """
        Cria matriz de caracter√≠sticas combinando m√∫ltiplas colunas relevantes
        """
        print("\nüîß Criando matriz de caracter√≠sticas...")
        
        # Colunas mais relevantes para an√°lise
        text_columns = [
            'Empenho (Hist√≥rico)(EOF)',
            'Fun√ß√£o (Cod/Nome)(EOF)',
            'Subfun√ß√£o (Cod/Nome)(EOF)',
            'A√ß√£o (Cod/Nome)(EOF)',
            'Programa (Cod/Nome)(EOF)',
            'Elemento Despesa (Cod/Nome)(EOF)',
            '√ìrg√£o (C√≥digo/Nome)(EOF)'
        ]
        
        # Combina textos de m√∫ltiplas colunas
        self.df['text_combined'] = ''
        for col in text_columns:
            if col in self.df.columns:
                self.df['text_combined'] += ' ' + self.df[col].fillna('').astype(str)
        
        # Pr√©-processa o texto combinado
        self.df['text_processed'] = self.df['text_combined'].apply(self.preprocess_text)
        
        # Vetoriza√ß√£o TF-IDF
        print("  ‚Ä¢ Aplicando TF-IDF...")
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),  # Unigrams, bigrams e trigrams
            min_df=2,
            max_df=0.95,
            use_idf=True,
            sublinear_tf=True  # Aplica log(tf + 1)
        )
        
        tfidf_matrix = self.vectorizer.fit_transform(self.df['text_processed'])

        # Usa apenas caracter√≠sticas textuais (TF-IDF)
        self.features_matrix = tfidf_matrix.toarray()

        print(f"‚úì Matriz de caracter√≠sticas criada: {self.features_matrix.shape}")
        print(f"  ‚Ä¢ Caracter√≠sticas baseadas apenas em texto (TF-IDF)")
        return self
    
    def cluster_dbscan(self):
        """
        Aplica DBSCAN para clustering com alta similaridade.
        Gera clusters para rotula√ß√£o manual posterior.
        """
        print(f"\nüéØ Aplicando DBSCAN com vigil√¢ncia œÅ ‚â• {self.vigilance}...")

        # Inicializa labels com -1 (n√£o rotulado)
        self.labels = np.full(len(self.df), -1)
        self.confidence_scores = np.zeros(len(self.df))

        # Calcula dist√¢ncia epsilon baseada na vigil√¢ncia
        # Vigil√¢ncia de 0.9 significa similaridade m√≠nima de 90%
        # Dist√¢ncia = 1 - similaridade
        eps = 1 - self.vigilance

        # DBSCAN clustering
        dbscan = DBSCAN(
            eps=eps,
            min_samples=2,  # M√≠nimo de 2 pontos para formar um cluster (mais granular)
            metric='cosine',  # M√©trica de cosseno para dados textuais
            n_jobs=-1
        )

        cluster_labels = dbscan.fit_predict(self.features_matrix)

        # Estat√≠sticas do clustering
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)

        print(f"  ‚Ä¢ Clusters encontrados: {n_clusters}")
        print(f"  ‚Ä¢ Pontos de ru√≠do (n√£o agrupados): {n_noise}")
        print(f"  ‚Ä¢ Clusters prontos para rotula√ß√£o manual")

        # Atualiza labels com os clusters encontrados
        for cluster_id in set(cluster_labels):
            if cluster_id != -1:  # Ignora ru√≠do por enquanto
                cluster_mask = (cluster_labels == cluster_id)
                if cluster_mask.sum() > 0:
                    # Usa o cluster_id diretamente como label
                    self.labels[cluster_mask] = cluster_id
                    # Confian√ßa inicial √© zero, pois ainda n√£o foram rotulados manualmente
                    self.confidence_scores[cluster_mask] = 0.0
        
        # Calcula m√©tricas de qualidade do clustering
        if n_clusters > 1:
            valid_points = cluster_labels != -1
            if valid_points.sum() > 0:
                try:
                    silhouette = silhouette_score(
                        self.features_matrix[valid_points], 
                        cluster_labels[valid_points],
                        metric='cosine'
                    )
                    print(f"  ‚Ä¢ Coeficiente de Silhueta: {silhouette:.3f}")
                except:
                    pass
        
        return self
    
    def semi_supervised_learning(self, n_iterations=5):
        """
        Aplica aprendizado semi-supervisionado iterativo
        
        Args:
            n_iterations: N√∫mero de itera√ß√µes
        """
        print(f"\nü§ñ Iniciando aprendizado semi-supervisionado ({n_iterations} itera√ß√µes)...")
        
        for iteration in range(n_iterations):
            print(f"\n  Itera√ß√£o {iteration + 1}/{n_iterations}")
            
            # Separa dados rotulados e n√£o rotulados
            labeled_mask = self.labels != -1
            n_labeled = labeled_mask.sum()
            n_unlabeled = (~labeled_mask).sum()
            
            print(f"    ‚Ä¢ Rotulados: {n_labeled}, N√£o rotulados: {n_unlabeled}")
            
            if n_labeled < 10 or n_unlabeled < 1:
                print("    ‚ö† Poucos dados para continuar")
                break
            
            # Label Propagation
            label_prop = LabelPropagation(
                kernel='rbf',
                gamma=20,
                max_iter=1000
            )
            
            # Prepara dados para treinamento
            labels_train = self.labels.copy()
            
            # Treina o modelo
            label_prop.fit(self.features_matrix, labels_train)
            
            # Obt√©m probabilidades de predi√ß√£o
            proba_predictions = label_prop.predict_proba(self.features_matrix)
            
            # Atualiza labels com alta confian√ßa
            confidence_threshold = 0.95 - (iteration * 0.05)  # Reduz threshold a cada itera√ß√£o
            confidence_threshold = max(confidence_threshold, 0.75)  # M√≠nimo de 75%
            
            new_labels = 0
            for i in range(len(self.df)):
                if self.labels[i] == -1:  # Apenas n√£o rotulados
                    max_proba = proba_predictions[i].max()
                    if max_proba >= confidence_threshold:
                        predicted_label = proba_predictions[i].argmax()
                        self.labels[i] = predicted_label
                        self.confidence_scores[i] = max_proba
                        new_labels += 1
            
            print(f"    ‚Ä¢ Novos r√≥tulos atribu√≠dos: {new_labels}")
            print(f"    ‚Ä¢ Threshold de confian√ßa: {confidence_threshold:.2f}")
            
            # Salva hist√≥rico da itera√ß√£o
            self.iteration_history.append({
                'iteration': iteration + 1,
                'n_labeled': n_labeled + new_labels,
                'n_unlabeled': n_unlabeled - new_labels,
                'new_labels': new_labels,
                'confidence_threshold': confidence_threshold
            })
            
            if new_labels == 0:
                print("    ‚ö† Nenhum novo r√≥tulo atribu√≠do, parando itera√ß√µes")
                break
        
        return self
    
    def analyze_results(self):
        """
        Analisa e visualiza os resultados da rotula√ß√£o
        """
        print("\nüìà Analisando resultados...")
        
        # Adiciona labels ao dataframe
        self.df['label'] = self.labels
        self.df['confidence'] = self.confidence_scores
        self.df['label_name'] = self.df['label'].map(self.label_mapping).fillna('CLUSTER_' + self.df['label'].astype(str))
        
        # Estat√≠sticas gerais
        print("\nüìä ESTAT√çSTICAS GERAIS:")
        print(f"  ‚Ä¢ Total de registros: {len(self.df)}")
        print(f"  ‚Ä¢ Registros rotulados: {(self.labels != -1).sum()} ({(self.labels != -1).sum()/len(self.df)*100:.1f}%)")
        print(f"  ‚Ä¢ Registros n√£o rotulados: {(self.labels == -1).sum()} ({(self.labels == -1).sum()/len(self.df)*100:.1f}%)")
        print(f"  ‚Ä¢ Confian√ßa m√©dia: {self.confidence_scores[self.labels != -1].mean():.3f}")
        
        # Distribui√ß√£o por label
        print("\nüìä DISTRIBUI√á√ÉO POR R√ìTULO:")
        label_counts = self.df[self.df['label'] != -1]['label_name'].value_counts()
        for label, count in label_counts.head(20).items():
            print(f"  ‚Ä¢ {label}: {count} ({count/len(self.df)*100:.1f}%)")
        
        # Criar visualiza√ß√µes
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Distribui√ß√£o de labels
        ax = axes[0, 0]
        top_labels = label_counts.head(15)
        ax.barh(range(len(top_labels)), top_labels.values)
        ax.set_yticks(range(len(top_labels)))
        ax.set_yticklabels(top_labels.index, fontsize=9)
        ax.set_xlabel('Quantidade')
        ax.set_title('Top 15 R√≥tulos Mais Frequentes')
        ax.grid(True, alpha=0.3)
        
        # 2. Evolu√ß√£o do aprendizado semi-supervisionado
        if self.iteration_history:
            ax = axes[0, 1]
            iterations = [h['iteration'] for h in self.iteration_history]
            labeled = [h['n_labeled'] for h in self.iteration_history]
            ax.plot(iterations, labeled, marker='o', linewidth=2, markersize=8)
            ax.set_xlabel('Itera√ß√£o')
            ax.set_ylabel('Registros Rotulados')
            ax.set_title('Evolu√ß√£o do Aprendizado Semi-Supervisionado')
            ax.grid(True, alpha=0.3)
        
        # 3. Distribui√ß√£o de confian√ßa
        ax = axes[0, 2]
        conf_data = self.confidence_scores[self.labels != -1]
        ax.hist(conf_data, bins=20, edgecolor='black', alpha=0.7)
        ax.axvline(conf_data.mean(), color='red', linestyle='--', label=f'M√©dia: {conf_data.mean():.2f}')
        ax.set_xlabel('Confian√ßa')
        ax.set_ylabel('Frequ√™ncia')
        ax.set_title('Distribui√ß√£o de Confian√ßa dos R√≥tulos')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # 4. PCA 2D dos clusters
        ax = axes[1, 0]
        pca = PCA(n_components=2, random_state=42)
        features_2d = pca.fit_transform(self.features_matrix)
        
        # Plot por label
        unique_labels = np.unique(self.labels[self.labels != -1])
        colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))
        
        for label, color in zip(unique_labels[:20], colors):  # Limita a 20 labels para visualiza√ß√£o
            mask = self.labels == label
            ax.scatter(features_2d[mask, 0], features_2d[mask, 1], 
                      c=[color], alpha=0.6, s=10, 
                      label=self.label_mapping.get(label, f'Cluster {label}')[:20])
        
        # Pontos n√£o rotulados
        mask_unlabeled = self.labels == -1
        ax.scatter(features_2d[mask_unlabeled, 0], features_2d[mask_unlabeled, 1],
                  c='gray', alpha=0.3, s=5, label='N√£o rotulado')
        
        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var)')
        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var)')
        ax.set_title('Visualiza√ß√£o PCA 2D dos Clusters')
        ax.legend(fontsize=6, loc='center left', bbox_to_anchor=(1, 0.5))
        
        # 5. Matriz de valores por √≥rg√£o
        ax = axes[1, 1]
        orgao_label = pd.crosstab(
            self.df['√ìrg√£o (C√≥digo/Nome)(EOF)'].str[:30],  # Trunca para caber
            self.df['label_name'].str[:20],
            values=self.df['Valor Empenhado (EOF)'],
            aggfunc='sum'
        ).fillna(0)
        
        # Top 10 √≥rg√£os por valor
        top_orgaos = orgao_label.sum(axis=1).nlargest(10).index
        top_labels_cols = orgao_label.sum(axis=0).nlargest(10).index
        
        subset = orgao_label.loc[top_orgaos, top_labels_cols]
        im = ax.imshow(subset.values, aspect='auto', cmap='YlOrRd')
        ax.set_xticks(range(len(top_labels_cols)))
        ax.set_xticklabels(top_labels_cols, rotation=45, ha='right', fontsize=8)
        ax.set_yticks(range(len(top_orgaos)))
        ax.set_yticklabels(top_orgaos, fontsize=8)
        ax.set_title('Heatmap: Valor Empenhado por √ìrg√£o e R√≥tulo')
        plt.colorbar(im, ax=ax)
        
        # 6. Temporal
        ax = axes[1, 2]
        self.df['mes'] = pd.to_datetime(self.df['Per√≠odo (Dia/Mes/Ano)(EOF)']).dt.to_period('M')
        temporal = self.df.groupby(['mes', 'label_name'])['Valor Empenhado (EOF)'].sum().reset_index()
        
        for label in temporal['label_name'].unique()[:5]:  # Top 5 labels
            data = temporal[temporal['label_name'] == label]
            ax.plot(data['mes'].astype(str), data['Valor Empenhado (EOF)'], 
                   marker='o', label=label[:30])
        
        ax.set_xlabel('Per√≠odo')
        ax.set_ylabel('Valor Empenhado (R$)')
        ax.set_title('Evolu√ß√£o Temporal dos Top 5 R√≥tulos')
        ax.legend(fontsize=8)
        ax.grid(True, alpha=0.3)
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        plt.tight_layout()
        plt.savefig('analise_rotulacao.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        return self
    
    def export_results(self, output_path):
        """
        Exporta os resultados para arquivo Excel
        """
        print(f"\nüíæ Exportando resultados para {output_path}...")
        
        # Prepara dataframe de sa√≠da
        output_df = self.original_df.copy()
        output_df['LABEL_ID'] = self.labels
        output_df['LABEL_NAME'] = self.df['label_name']
        output_df['CONFIDENCE_SCORE'] = self.confidence_scores
        output_df['LABELED'] = (self.labels != -1).astype(int)
        
        # Ordena por confian√ßa e label
        output_df = output_df.sort_values(['LABELED', 'CONFIDENCE_SCORE'], ascending=[False, False])
        
        # Salva em Excel com m√∫ltiplas abas
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Aba principal com todos os dados
            output_df.to_excel(writer, sheet_name='Dados Rotulados', index=False)
            
            # Aba de estat√≠sticas
            stats_df = pd.DataFrame([
                {'M√©trica': 'Total de Registros', 'Valor': len(output_df)},
                {'M√©trica': 'Registros Rotulados', 'Valor': (self.labels != -1).sum()},
                {'M√©trica': 'Registros N√£o Rotulados', 'Valor': (self.labels == -1).sum()},
                {'M√©trica': 'Percentual Rotulado', 'Valor': f"{(self.labels != -1).sum()/len(output_df)*100:.2f}%"},
                {'M√©trica': 'Confian√ßa M√©dia', 'Valor': f"{self.confidence_scores[self.labels != -1].mean():.3f}"},
                {'M√©trica': 'Total de R√≥tulos √önicos', 'Valor': len(np.unique(self.labels[self.labels != -1]))}
            ])
            stats_df.to_excel(writer, sheet_name='Estat√≠sticas', index=False)
            
            # Aba com resumo por label
            summary_df = output_df[output_df['LABELED'] == 1].groupby('LABEL_NAME').agg({
                'LABEL_ID': 'count',
                'CONFIDENCE_SCORE': 'mean',
                'Valor Empenhado (EOF)': 'sum'
            }).round(2)
            summary_df.columns = ['Quantidade', 'Confian√ßa M√©dia', 'Valor Total Empenhado']
            summary_df = summary_df.sort_values('Quantidade', ascending=False)
            summary_df.to_excel(writer, sheet_name='Resumo por R√≥tulo')
            
            # Aba com hist√≥rico de itera√ß√µes
            if self.iteration_history:
                history_df = pd.DataFrame(self.iteration_history)
                history_df.to_excel(writer, sheet_name='Hist√≥rico Itera√ß√µes', index=False)
        
        print(f"‚úì Resultados exportados com sucesso!")
        return output_path

# Execu√ß√£o principal
def main():
    """Fun√ß√£o principal para executar o pipeline completo"""
    
    print("="*80)
    print("üöÄ SISTEMA DE ROTULA√á√ÉO SEMI-SUPERVISIONADA DE DADOS OR√áAMENT√ÅRIOS")
    print("="*80)

    # Inicializa o sistema com vigil√¢ncia de 0.9
    labeler = SemiSupervisedBudgetLabeler(vigilance=0.9)

    # Pipeline completo - Primeira etapa: Clustering
    (labeler
        .load_data('siof_saude.xlsx')
        .create_features()
        .cluster_dbscan()
        .analyze_results()
        .export_results('dados_clusters.xlsx')
    )

    print("\n" + "="*80)
    print("‚úÖ CLUSTERING CONCLU√çDO COM SUCESSO!")
    print("="*80)

    # Recomenda√ß√µes finais
    print("\nüìã PR√ìXIMOS PASSOS RECOMENDADOS:")
    print("1. Revise os clusters gerados na planilha 'dados_clusters.xlsx'")
    print("2. Rotule MANUALMENTE alguns exemplos de cada cluster principal")
    print("3. Salve os dados rotulados e execute o aprendizado semi-supervisionado")
    print("4. Use .semi_supervised_learning() para propagar os r√≥tulos manuais")
    print("5. Ajuste o par√¢metro de vigil√¢ncia se necess√°rio (atual: 0.9 = 90% similaridade)")
    
    return labeler

if __name__ == "__main__":
    labeler = main()
